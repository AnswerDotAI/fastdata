{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Core functionality for synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import concurrent.futures\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "from typing import Optional, Union\n",
    "\n",
    "from tqdm import tqdm\n",
    "from fastcore.utils import *\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from huggingface_hub import CommitScheduler, DatasetCard\n",
    "from claudette import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DATASET_CARD_TEMPLATE = \"\"\"\n",
    "---\n",
    "tags:\n",
    "- fastdata\n",
    "- synthetic\n",
    "---\n",
    "\n",
    "# {title}\n",
    "\n",
    "_Note: This is an AI-generated dataset, so its content may be inaccurate or false._\n",
    "\n",
    "**Source of the data:**\n",
    "\n",
    "The dataset was generated using [Fastdata](https://github.com/AnswerDotAI/fastdata) library and {model_id} with the following input:\n",
    "\n",
    "## System Prompt\n",
    "\n",
    "```\n",
    "{system_prompt}\n",
    "```\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "```\n",
    "{prompt_template}\n",
    "```\n",
    "\n",
    "## Sample Input\n",
    "\n",
    "```json\n",
    "{sample_input}\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class FastData:\n",
    "    def __init__(self,\n",
    "                 model: str = \"claude-3-haiku-20240307\",\n",
    "                 calls: int = 100,\n",
    "                 period: int = 60):\n",
    "        self.cli = Client(model)\n",
    "        self._set_rate_limit(calls, period)\n",
    "\n",
    "    def _set_rate_limit(self, calls: int, period: int):\n",
    "        \"\"\"Set a new rate limit.\"\"\"\n",
    "        @sleep_and_retry\n",
    "        @limits(calls=calls, period=period)\n",
    "        def rate_limited_call(prompt: str, schema, temp: float, sp: str):\n",
    "            return self.cli.structured(\n",
    "                prompt,\n",
    "                temp=temp,\n",
    "                tools=schema,\n",
    "            )[0]\n",
    "        \n",
    "        self._rate_limited_call = rate_limited_call\n",
    "\n",
    "    def _process_input(self, prompt_template, schema, temp, sp, input_data):\n",
    "        try:\n",
    "            prompt = prompt_template.format(**input_data)\n",
    "            return self._rate_limited_call(\n",
    "                prompt=prompt, schema=schema, temp=temp, sp=sp\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing input {input_data}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _save_results(self, results: list[dict], save_path: Path) -> None:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            for res in results:\n",
    "                obj_dict = getattr(res, \"__stored_args__\", res.__dict__)\n",
    "                f.write(json.dumps(obj_dict) + \"\\n\")\n",
    "\n",
    "    def generate(self, \n",
    "                 prompt_template: str, \n",
    "                 inputs: list[dict], \n",
    "                 schema,\n",
    "                 temp: float = 1.,\n",
    "                 sp: str = \"You are a helpful assistant.\",\n",
    "                 max_workers: int = 64) -> list[dict]:\n",
    "        \"For every input in INPUTS, fill PROMPT_TEMPLATE and generate a value fitting SCHEMA\"\n",
    "        \n",
    "        with tqdm(total=len(inputs)) as pbar:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                futures = [\n",
    "                        executor.submit(\n",
    "                            self._process_input,\n",
    "                            prompt_template,\n",
    "                            schema,\n",
    "                            temp,\n",
    "                            sp,\n",
    "                            input_data,\n",
    "                        )\n",
    "                        for input_data in inputs\n",
    "                    ]\n",
    "\n",
    "                for completed_future in concurrent.futures.as_completed(futures):\n",
    "                    pbar.update(1)\n",
    "        return [f.result() for f in futures]\n",
    "\n",
    "    def generate_to_hf(\n",
    "        self,\n",
    "        prompt_template: str,\n",
    "        inputs: list[dict],\n",
    "        schema,\n",
    "        repo_id: str,\n",
    "        temp: float = 1.0,\n",
    "        sp: str = \"You are a helpful assistant.\",\n",
    "        max_workers: int = 64,\n",
    "        max_items_per_file: int = 100,\n",
    "        commit_every: Union[int, float] = 5,\n",
    "        private: Optional[bool] = None,\n",
    "        token: Optional[str] = None,\n",
    "        delete_files_after: bool = True,\n",
    "    ) -> tuple[str, list[dict]]:\n",
    "        \"\"\"\n",
    "        Generate data based on a prompt template and schema, and save it to Hugging Face dataset repository.\n",
    "        This function writes the generated records to multiple files, each containing a maximum of `max_items_per_file` records. \n",
    "        Due to the multi-threaded execution of the function, the order of the records in the files is not guaranteed to match the order of the input data. \n",
    "\n",
    "        Args:\n",
    "            prompt_template (str): The template for generating prompts.\n",
    "            inputs (list[dict]): A list of input dictionaries to be processed.\n",
    "            schema: The schema to parse the generated data.\n",
    "            repo_id (str): The HuggingFace dataset name.\n",
    "            temp (float, optional): The temperature for generation. Defaults to 1.0.\n",
    "            sp (str, optional): The system prompt for the assistant. Defaults to \"You are a helpful assistant.\".\n",
    "            max_workers (int, optional): The maximum number of worker threads. Defaults to 64.\n",
    "            max_items_per_file (int, optional): The maximum number of items to save in each file. Defaults to 100.\n",
    "            commit_every (Union[int, float], optional): The number of minutes between each commit. Defaults to 5.\n",
    "            private (bool, optional): Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
    "            token (Optional[str], optional): The token to use to commit to the repo. Defaults to the token saved on the machine.\n",
    "            delete_files_after (bool, optional): Whether to delete files after processing. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            tuple[str, list[dict]]: A tuple with the generated repo_id and the list of generated data dictionaries.\n",
    "        \"\"\"\n",
    "        dataset_dir = Path(repo_id.replace(\"/\", \"_\"))\n",
    "        dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "        data_dir = dataset_dir / \"data\"\n",
    "        data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            scheduler = CommitScheduler(\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"dataset\",\n",
    "                folder_path=dataset_dir,\n",
    "                every=commit_every,\n",
    "                private=private,\n",
    "                token=token,\n",
    "            )\n",
    "\n",
    "            readme_path = dataset_dir / \"README.md\"\n",
    "\n",
    "            if not readme_path.exists():\n",
    "                DatasetCard(\n",
    "                    DATASET_CARD_TEMPLATE.format(\n",
    "                        title=repo_id,\n",
    "                        model_id=self.cli.model,\n",
    "                        system_prompt=sp,\n",
    "                        prompt_template=prompt_template,\n",
    "                        sample_input=inputs[:2],\n",
    "                    )\n",
    "                ).save(readme_path)\n",
    "\n",
    "            results = []\n",
    "            total_inputs = len(inputs)\n",
    "\n",
    "            with tqdm(total=total_inputs) as pbar:\n",
    "                with concurrent.futures.ThreadPoolExecutor(\n",
    "                    max_workers=max_workers\n",
    "                ) as executor:\n",
    "                    futures = [\n",
    "                        executor.submit(\n",
    "                            self._process_input,\n",
    "                            prompt_template,\n",
    "                            schema,\n",
    "                            temp,\n",
    "                            sp,\n",
    "                            input_data,\n",
    "                        )\n",
    "                        for input_data in inputs\n",
    "                    ]\n",
    "\n",
    "                    current_file = data_dir / f\"train-{uuid4()}.jsonl\"\n",
    "                    for completed_future in concurrent.futures.as_completed(futures):\n",
    "                        result = completed_future.result()\n",
    "                        if result is not None:\n",
    "                            results.append(result)\n",
    "                            with scheduler.lock:\n",
    "                                self._save_results(results, current_file)\n",
    "                        pbar.update(1)\n",
    "                        if len(results) >= max_items_per_file:\n",
    "                            current_file = data_dir / f\"train-{uuid4()}.jsonl\"\n",
    "                            results.clear()\n",
    "        finally:\n",
    "            scheduler.trigger().result()  # force upload last result\n",
    "            if delete_files_after:\n",
    "                shutil.rmtree(dataset_dir)\n",
    "\n",
    "        return scheduler.repo_id, [f.result() for f in futures if f.done()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def to_md(ss): return '\\n'.join(f'- {s}' for s in ss) \n",
    "def show(ss): return Markdown(to_md(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello, how are you today? ➡ *Hola, ¿cómo estás hoy?*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Translation():\n",
    "    \"Translation from an English phrase to a Spanish phrase\"\n",
    "    def __init__(self, english: str, spanish: str): store_attr()\n",
    "    def __repr__(self): return f\"{self.english} ➡ *{self.spanish}*\"\n",
    "\n",
    "Translation(\"Hello, how are you today?\", \"Hola, ¿cómo estás hoy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Hello, my name is Nathan. I am a research scientist at an AI startup. ➡ *Hola, me llamo Nathan. Soy ciencia investigador en un startup de IA.*\n",
       "- How much wood could a woodchuck chuck if a woodchuck could chuck wood? ➡ *¿Cuánta madera podría arrojar una marmota si una marmota pudiera arrojar madera?*\n",
       "- Thomas Cranmer (2 July 1489 - 21 March 1556) was a leader of the English Reformation and Archbishop of Canterbury during the reigns of Henry VIII, Edward VI and, for a short time, Mary I. He helped build the case for the annulment of Henry's marriage to Catherine of Aragon, which was one of the causes of the separation of the English Church from union with the Holy See. ➡ *Thomas Cranmer (2 de julio de 1489 - 21 de marzo de 1556) fue un líder de la Reforma inglesa y arzobispo de Canterbury durante los reinados de Henry VIII, Edward VI y, por un corto tiempo, María I. Ayudó a construir el caso para la anulación de El matrimonio de Henry con Catalina de Aragón, que fue una de las causas de la separación de la Iglesia inglesa de la unión con la Santa Sede.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\n",
    "    Translation(\n",
    "        english=\"Hello, my name is Nathan. I am a research scientist at an AI startup.\",\n",
    "        spanish=\"Hola, me llamo Nathan. Soy ciencia investigador en un startup de IA.\"),\n",
    "    Translation(\n",
    "        english=\"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\",\n",
    "        spanish=\"¿Cuánta madera podría arrojar una marmota si una marmota pudiera arrojar madera?\"),\n",
    "    Translation(\n",
    "        english=\"Thomas Cranmer (2 July 1489 - 21 March 1556) was a leader of the English Reformation and Archbishop of Canterbury during the reigns of Henry VIII, Edward VI and, for a short time, Mary I. He helped build the case for the annulment of Henry's marriage to Catherine of Aragon, which was one of the causes of the separation of the English Church from union with the Holy See.\",\n",
    "        spanish=\"Thomas Cranmer (2 de julio de 1489 - 21 de marzo de 1556) fue un líder de la Reforma inglesa y arzobispo de Canterbury durante los reinados de Henry VIII, Edward VI y, por un corto tiempo, María I. Ayudó a construir el caso para la anulación de El matrimonio de Henry con Catalina de Aragón, que fue una de las causas de la separación de la Iglesia inglesa de la unión con la Santa Sede.\"\n",
    "    ),\n",
    "]\n",
    "show(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/algal/miniconda/envs/jup3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|█| 200000/200000 [00:00<00:00, 3852934.04 examples/\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "- A Political Analyst specialized in El Salvador's political landscape.\n",
       "- A legal advisor who understands the legal implications of incomplete or inaccurate project documentation\n",
       "- A maternal health advocate focused on raising awareness about postpartum complications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load personas\n",
    "personas = load_dataset(\"proj-persona/PersonaHub\", \"persona\", split='train').select(range(3))['persona']\n",
    "show(personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = \"You will help generate synthetic data of English and Spanish phrases.\"\n",
    "prompt_template = \"\"\"\\\n",
    "<examples>\n",
    "{examples}\n",
    "</examples>\n",
    "\n",
    "Create an English and Spanish translation pair that is similar to the examples and would be appropriate for the following persona:\n",
    "<persona>{persona}</persona>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the prompt looks like in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<examples>\n",
      "- Hello, my name is Nathan. I am a research scientist at an AI startup. ➡ *Hola, me llamo Nathan. Soy ciencia investigador en un startup de IA.*\n",
      "- How much wood could a woodchuck chuck if a woodchuck could chuck wood? ➡ *¿Cuánta madera podría arrojar una marmota si una marmota pudiera arrojar madera?*\n",
      "- Thomas Cranmer (2 July 1489 - 21 March 1556) was a leader of the English Reformation and Archbishop of Canterbury during the reigns of Henry VIII, Edward VI and, for a short time, Mary I. He helped build the case for the annulment of Henry's marriage to Catherine of Aragon, which was one of the causes of the separation of the English Church from union with the Holy See. ➡ *Thomas Cranmer (2 de julio de 1489 - 21 de marzo de 1556) fue un líder de la Reforma inglesa y arzobispo de Canterbury durante los reinados de Henry VIII, Edward VI y, por un corto tiempo, María I. Ayudó a construir el caso para la anulación de El matrimonio de Henry con Catalina de Aragón, que fue una de las causas de la separación de la Iglesia inglesa de la unión con la Santa Sede.*\n",
      "</examples>\n",
      "\n",
      "Create an English and Spanish translation pair that is similar to the examples and would be appropriate for the following persona:\n",
      "<persona>A Political Analyst specialized in El Salvador's political landscape.</persona>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples_md = to_md(examples)\n",
    "prompt = prompt_template.format(examples=examples_md, persona=personas[0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate translations\n",
    "fast_data = FastData(model=\"claude-3-haiku-20240307\")\n",
    "translations = fast_data.generate(\n",
    "    prompt_template=prompt_template,\n",
    "    inputs=[{\"persona\": persona, \"examples\": examples} for persona in personas],\n",
    "    schema=Translation,\n",
    "    sp=sp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- The political situation in El Salvador continues to be complex, with ongoing tensions between the ruling party and opposition groups. President Nayib Bukele has consolidated significant power, raising concerns about the state of democracy in the country. ➡ *La situación política en El Salvador sigue siendo compleja, con tensiones persistentes entre el partido gobernante y los grupos de oposición. El presidente Nayib Bukele ha consolidado un poder significativo, lo que genera preocupaciones sobre el estado de la democracia en el país.*\n",
       "- Thorough documentation is critical for any legal proceedings. Incomplete or inaccurate records can have serious consequences. ➡ *La documentación exhaustiva es fundamental para cualquier proceso legal. Los registros incompletos o inexactos pueden tener consecuencias graves.*\n",
       "- Postpartum complications can be life-threatening, but with proper care and support, new mothers can recover and thrive. Let's work together to ensure all women have access to the resources they need during this crucial time. ➡ *Las complicaciones posparto pueden poner en riesgo la vida, pero con la atención y el apoyo adecuados, las nuevas madres pueden recuperarse y prosperar. Trabajemos juntos para garantizar que todas las mujeres tengan acceso a los recursos que necesitan durante este momento crucial.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translations and push results to Hugging Face Hub as a dataset\n",
    "# Be sure to have the HF_TOKEN environment variable set to your Hugging Face API token\n",
    "fast_data = FastData(model=\"claude-3-haiku-20240307\")\n",
    "repo_id, translations = fast_data.generate_to_hf(\n",
    "    prompt_template=prompt_template,\n",
    "    inputs=[{\"persona\": persona, \"examples\": examples} for persona in personas],\n",
    "    schema=Translation,\n",
    "    sp=sp,\n",
    "    repo_id=f\"personas-translation-{uuid4()}\",\n",
    "    max_items_per_file=2, # It will create a local file each 2 translations  \n",
    ")\n",
    "assert len(translations) == len(personas)\n",
    "\n",
    "new_dataset = load_dataset(repo_id)\n",
    "assert len(translations) == len(personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationCritique():\n",
    "    \"A critique of the translation.\"\n",
    "    def __init__(self, critique: str, score: int): store_attr()\n",
    "    def __repr__(self): return f\"\\t- **Critique:** {self.critique}\\n\\t- **Score:** {self.score}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = \"You will help critique synthetic data of English and Spanish phrases.\"\n",
    "critique_template = \"\"\"\\\n",
    "Below is an extract of a translation. Evaluate its quality as a senior translator would, considering its suitability for professional use. Use the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion:\n",
    "\n",
    "- Add 1 point if the translation conveys the basic meaning of the source text, even if it includes some minor errors or awkward phrasing.\n",
    "- Add another point if the translation is generally accurate but lacks refinement in style or fails to capture some nuances of the original. It might use inconsistent terminology or have occasional lapses in register.\n",
    "- Award a third point if the translation is appropriate for professional use and accurately conveys key concepts of the source text. It demonstrates good understanding of both languages, though it may not be flawless or could include some slight inconsistencies. It resembles the work of a competent translator but may have room for improvement in fluency or precision.\n",
    "- Grant a fourth point if the translation is highly accurate and reads naturally in the target language, exhibiting a consistent and appropriate style. It could be similar to the work of an experienced translator, offering faithful rendering of content and tone, with minimal errors, and effectively handling complex concepts or cultural references. The result is coherent, well-expressed, and valuable for its intended purpose.\n",
    "- Bestow a fifth point if the translation is outstanding, demonstrating mastery of both source and target languages. It captures subtle nuances, maintains the author's voice and intent, and reads as if it were originally written in the target language. The translator has made excellent choices in dealing with challenging elements like wordplay, idiomatic expressions, or culture-specific content.\n",
    "\n",
    "<translation>{translation}</translation>\n",
    "\n",
    "After examining the translation:\n",
    "\n",
    "- Briefly justify your total score, up to 100 words.\n",
    "- Conclude with the score of the translation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "fast_data = FastData(model=\"claude-3-5-sonnet-20240620\")\n",
    "critiques = fast_data.generate(\n",
    "    prompt_template=critique_template,\n",
    "    inputs=[{\"translation\": f\"{t.english} -> {t.spanish}\"} for t in translations],\n",
    "    schema=TranslationCritique,\n",
    "    sp=sp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- The political situation in El Salvador continues to be complex, with ongoing tensions between the ruling party and opposition groups. President Nayib Bukele has consolidated significant power, raising concerns about the state of democracy in the country. ➡ *La situación política en El Salvador sigue siendo compleja, con tensiones persistentes entre el partido gobernante y los grupos de oposición. El presidente Nayib Bukele ha consolidado un poder significativo, lo que genera preocupaciones sobre el estado de la democracia en el país.*\n",
       "\n",
       "\t- **Critique:** The translation accurately conveys the meaning of the source text, capturing the complexity of El Salvador's political situation. It effectively translates key terms and maintains the tone of the original. The translator demonstrates a strong grasp of both languages, rendering the content naturally in Spanish. The phrasing is appropriate and flows well, preserving the nuances of the English version. There are no noticeable errors or awkward constructions. The translation skillfully handles the political terminology and concepts, making it suitable for professional use. It reads as if it were originally written in Spanish, indicating the translator's expertise.\n",
       "\t- **Score:** 5\n",
       "- Thorough documentation is critical for any legal proceedings. Incomplete or inaccurate records can have serious consequences. ➡ *La documentación exhaustiva es fundamental para cualquier proceso legal. Los registros incompletos o inexactos pueden tener consecuencias graves.*\n",
       "\n",
       "\t- **Critique:** The translation accurately conveys the main message of the source text, maintaining both the meaning and tone. It effectively captures the importance of thorough documentation in legal proceedings and the potential consequences of incomplete or inaccurate records. The translator has chosen appropriate Spanish equivalents for key terms, such as \"exhaustiva\" for \"thorough\" and \"proceso legal\" for \"legal proceedings\". The sentence structure is natural in Spanish, and the translation maintains the formal register suitable for legal contexts. There are no errors in grammar or vocabulary. The translation demonstrates a high level of competence, accurately conveying complex concepts while reading naturally in the target language. It could be considered the work of an experienced translator.\n",
       "\t- **Score:** 4\n",
       "- Postpartum complications can be life-threatening, but with proper care and support, new mothers can recover and thrive. Let's work together to ensure all women have access to the resources they need during this crucial time. ➡ *Las complicaciones posparto pueden poner en riesgo la vida, pero con la atención y el apoyo adecuados, las nuevas madres pueden recuperarse y prosperar. Trabajemos juntos para garantizar que todas las mujeres tengan acceso a los recursos que necesitan durante este momento crucial.*\n",
       "\n",
       "\t- **Critique:** The translation accurately conveys the meaning of the original text, maintaining both the informative and encouraging tone. It correctly translates key terms like \"postpartum complications\" and \"life-threatening.\" The Spanish version flows naturally and captures the nuances of the original, including the call to action. The translator has made excellent choices in vocabulary and structure, resulting in a text that reads as if it were originally written in Spanish. The translation demonstrates a high level of proficiency in both languages and would be suitable for professional use in healthcare communications.\n",
       "\t- **Score:** 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(f'{t}\\n\\n{c}' for t, c in zip(translations, critiques))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that generate outputs align with inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the `FastData.generate` returns results in the same order as the inputs it was passed.\n",
    "\n",
    "To show this, we will define a new prompt template, where the model is asked only to echo a piece of data\n",
    "from the input. Then we will verify that the values in the inputs matches the values in the outputs, in order and in value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=\"You will help with accurate and faithful data processing.\"\n",
    "prompt_template = \"\"\"\\\n",
    "Below you find an item of data, a datum, which is an alphanumeric string:\n",
    "\n",
    "<datum>{datum}</datum>\n",
    "\n",
    "After reviewing this datum, please echo is back exactly, without any preamble:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datum:\n",
    "    \"A data value\"\n",
    "    def __init__(self, datum: str): store_attr()\n",
    "    def __repr__(self): return f\"{self.datum}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll test that the prompt and schema class work as execpted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below you find an item of data, a datum, which is an alphanumeric string:\n",
      "\n",
      "<datum>b9121446-e46c-47c0-9e6d-b4df35c0974b</datum>\n",
      "\n",
      "After reviewing this datum, please echo is back exactly, without any preamble:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(**dict(datum=str(uuid4()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "04da7de4-cc39-4699-9d25-5a476e366732"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Datum(str(uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ask the model to \"generate\" (i.e., echo) 100 of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:04<00:00, 24.17it/s]\n"
     ]
    }
   ],
   "source": [
    "in_vals = [{\"datum\":str(uuid4())} for _ in range(100)]\n",
    "out_vals = fast_data.generate(\n",
    "    prompt_template=prompt_template,\n",
    "    inputs=in_vals,\n",
    "    schema=Datum,\n",
    "    sp=sp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see if the inputs and outputs are aligned.\n",
    "\n",
    "If they are aligned, then that shows two things. First it shows that the model is echoing the values faithfully. Second, it shows _either_ that the model itself returned outputs in the order in which they were submitted, or else that `generate` has returned outputs in submission order.\n",
    "\n",
    "We are submitting a large enough quantity of items, that we _asssume_ the model will return some results out of submission order. If you want confidence which does not depend on this assumption, then could modify the test above to increase the number and complexity of the generation task, or simply inspect the implementation.\n",
    "\n",
    "Let's start by spot checking the first item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f42ea0db-24ce-4e09-a50d-edf74d0eb611',\n",
       " 'f42ea0db-24ce-4e09-a50d-edf74d0eb611')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_list = [x['datum'] for x in in_vals]\n",
    "out_list = [x.datum for x in out_vals]\n",
    "(in_list[0],out_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: output items are aligned with input items\n"
     ]
    }
   ],
   "source": [
    "for (idx,(in_item,out_item)) in enumerate(zip(in_list,out_list)):\n",
    "    if in_item != out_item:\n",
    "        print(\"Failure: output items were not aligned with input items!\")\n",
    "        print(f\"\\titem {idx} had in={in_item} and out={out_item}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Success: output items are aligned with input items\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
